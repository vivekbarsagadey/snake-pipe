# Snake-Pipe: AST Processing ETL Pipeline

## ğŸ“‹ Project Overview

Snake-Pipe is a high-performance ETL (Extract, Transform, Load) pipeline designed specifically for processing Abstract Syntax Tree (AST) JSON files generated by language-specific parsers. The system provides a robust, scalable, and pluggable architecture for ingesting, preprocessing, and storing code analysis data across multiple database backends.

## ğŸ¯ Project Vision

To create a unified data processing pipeline that seamlessly handles AST JSON outputs from various programming language parsers, providing clean, normalized, and enriched code analysis data stored across multiple specialized database systems for different use cases.

## ğŸ—ï¸ High-Level Architecture

### Core Data Flow

```
AST JSON Folder Structure
â”œâ”€â”€ Language-specific subdirectories
â”œâ”€â”€ Multiple JSON files mirroring source code structure
â””â”€â”€ Independent parser team outputs
            â”‚
            â–¼
[Optional] File Watcher Service
â”œâ”€â”€ Real-time monitoring of AST JSON folder
â”œâ”€â”€ Event-driven processing triggers
â””â”€â”€ Immediate routing to preprocessing layer
            â”‚
            â–¼
Preprocessing Layer (Core ETL Engine)
â”œâ”€â”€ Schema Validation (JSON Schema/Pydantic)
â”œâ”€â”€ Data Normalization & Standardization
â”œâ”€â”€ Cross-file Linking & Enrichment
â”œâ”€â”€ Deduplication & Conflict Resolution
â””â”€â”€ Error Handling & Quarantine Management
            â”‚
            â–¼
Database Manager (Plugin Architecture)
â”œâ”€â”€ Configuration-driven backend selection
â”œâ”€â”€ Multi-database write coordination
â”œâ”€â”€ Batch processing & optimization
â”œâ”€â”€ Retry mechanisms & fault tolerance
â””â”€â”€ Transaction management across backends
            â”‚
            â–¼
Pluggable Database Backends
â”œâ”€â”€ NebulaGraph (Graph relationships & dependencies)
â”œâ”€â”€ PostgreSQL (Structured relational data)
â”œâ”€â”€ Vector Database (Semantic search capabilities)
â”œâ”€â”€ Elasticsearch (Full-text search & analytics)
â””â”€â”€ Future backends (MongoDB, ClickHouse, etc.)
            â”‚
            â–¼
Ingestion Tracking System
â”œâ”€â”€ SQLite/PostgreSQL logging database
â”œâ”€â”€ Processing status & metadata tracking
â”œâ”€â”€ Timestamp & destination recording
â””â”€â”€ Duplicate prevention & audit trails
```

## ğŸ¯ Key Business Objectives

### Primary Goals
- **Unified Data Processing**: Standardize AST data from multiple programming languages
- **Multi-Database Support**: Enable specialized storage for different analytical use cases
- **Real-time Processing**: Support both batch and streaming data ingestion modes
- **Data Quality Assurance**: Ensure clean, validated, and enriched data storage
- **Scalability**: Handle large codebases with millions of files efficiently

### Success Metrics
- **Processing Throughput**: Handle 10,000+ JSON files per minute
- **Data Quality**: 99.9% successful validation and normalization rate
- **System Availability**: 99.5% uptime for continuous processing
- **Storage Efficiency**: Optimized data distribution across multiple backends
- **Recovery Time**: <5 minutes for full system recovery from failures

## ğŸ›ï¸ System Architecture Principles

### Design Philosophy
- **Modularity**: Each component is independently deployable and testable
- **Pluggability**: Easy addition of new database backends and processing modules
- **Fault Tolerance**: Graceful handling of failures with comprehensive recovery mechanisms
- **Configuration-Driven**: Behavior controlled through declarative configuration files
- **Observability**: Comprehensive logging, monitoring, and alerting throughout the pipeline

### Architecture Patterns
- **ETL Pipeline Pattern**: Clear Extract-Transform-Load separation of concerns
- **Plugin Architecture**: Extensible backend system with standardized interfaces
- **Event-Driven Processing**: Reactive system responding to file system changes
- **Batch & Stream Processing**: Hybrid approach supporting both processing modes
- **Circuit Breaker Pattern**: Fault isolation and recovery mechanisms

## ğŸ“Š Data Processing Stages

### 1. Extract Phase
**Purpose**: Discover and collect AST JSON files from the input directory structure

**Responsibilities**:
- Monitor designated AST JSON folder structure
- Detect new and modified JSON files
- Maintain file system state and change detection
- Route files to preprocessing layer efficiently

**Key Features**:
- Real-time file watching capabilities
- Batch processing for initial large-scale ingestion
- File integrity validation before processing
- Support for nested directory structures mirroring source code organization

### 2. Transform Phase (Preprocessing Layer)
**Purpose**: Validate, normalize, and enrich AST JSON data for optimal storage

**Core Components**:

#### Schema Validation
- JSON Schema validation against language-specific AST formats
- Pydantic model validation for type safety and data integrity
- Custom validation rules for cross-language consistency
- Error reporting and invalid data quarantine

#### Data Normalization
- Identifier standardization across different programming languages
- Naming convention harmonization
- Language tag assignment and metadata enrichment
- File path and location normalization

#### Cross-File Enrichment
- Dependency relationship discovery and linking
- Import/export relationship mapping
- Call graph construction and optimization
- Symbol reference resolution across files

#### Deduplication & Conflict Resolution
- Content-based duplicate detection
- Timestamp-based conflict resolution
- Merge strategies for overlapping data
- Version control integration for source tracking

### 3. Load Phase (Database Manager)
**Purpose**: Efficiently distribute processed data across multiple specialized database backends

**Core Capabilities**:
- Configuration-driven backend selection and routing
- Parallel writes to multiple database systems
- Transaction coordination across different database types
- Batch optimization for high-throughput scenarios
- Automatic retry and error recovery mechanisms

## ğŸ”§ Database Backend Strategy

### NebulaGraph (Graph Database)
**Use Case**: Code dependency relationships, call graphs, and complex interconnections
**Data Types**: 
- Node relationships between classes, functions, and modules
- Import/export dependency chains
- Inheritance hierarchies and composition patterns
- Cross-language interface definitions

### PostgreSQL (Relational Database)
**Use Case**: Structured queries, reporting, and transactional data integrity
**Data Types**:
- Normalized code metrics and statistics
- File metadata and processing history
- User access patterns and query logs
- Configuration and system state data

### Vector Database
**Use Case**: Semantic search, code similarity, and AI-powered analysis
**Data Types**:
- Code embeddings for semantic similarity
- Natural language descriptions and comments
- Pattern matching and recommendation data
- Machine learning feature vectors

### Elasticsearch (Search & Analytics)
**Use Case**: Full-text search, real-time analytics, and dashboard creation
**Data Types**:
- Searchable code content and documentation
- Log aggregation and analysis
- Performance metrics and monitoring data
- User activity and system usage patterns

## ğŸ“ˆ Operational Excellence

### Monitoring & Observability
- **Processing Metrics**: Throughput, latency, and error rates
- **System Health**: Resource utilization, queue depths, and response times
- **Data Quality**: Validation success rates, transformation accuracy
- **Business Metrics**: File processing counts, database growth, user activity

### Error Handling & Recovery
- **Graceful Degradation**: Continue processing when individual backends fail
- **Quarantine Management**: Isolate problematic files for manual review
- **Automatic Retry**: Configurable retry strategies with exponential backoff
- **Dead Letter Queues**: Capture and analyze repeatedly failing items

### Security & Compliance
- **Data Encryption**: End-to-end encryption for sensitive code data
- **Access Control**: Role-based permissions for different user types
- **Audit Logging**: Comprehensive tracking of all data access and modifications
- **Privacy Protection**: Anonymization options for sensitive source code

## ğŸš€ Scalability & Performance

### Horizontal Scaling
- **Microservice Architecture**: Independent scaling of processing components
- **Load Balancing**: Distribute processing across multiple worker nodes
- **Queue Management**: Message queues for asynchronous processing coordination
- **Database Sharding**: Partition data across multiple database instances

### Performance Optimization
- **Batch Processing**: Optimize database writes through intelligent batching
- **Caching Strategy**: Multi-level caching for frequently accessed data
- **Index Optimization**: Database-specific indexing strategies for query performance
- **Compression**: Efficient storage and transfer of large JSON datasets

## ğŸ”„ Development & Deployment

### Configuration Management
- **Environment-Specific Configs**: Development, staging, and production configurations
- **Database Backend Selection**: Runtime configuration of active backends
- **Processing Rules**: Customizable validation and transformation rules
- **Feature Flags**: Enable/disable features without code deployment

### Testing Strategy
- **Unit Testing**: Comprehensive coverage of individual components
- **Integration Testing**: End-to-end pipeline validation with sample data
- **Performance Testing**: Load testing with realistic data volumes
- **Chaos Engineering**: Fault injection testing for resilience validation

### Deployment Pipeline
- **Containerization**: Docker-based deployment for consistency
- **Infrastructure as Code**: Automated provisioning of required resources
- **Blue-Green Deployment**: Zero-downtime deployment strategy
- **Rollback Capabilities**: Quick recovery from problematic deployments



## ğŸ“ Technical Standards

### Code Quality
- **Test Coverage**: Minimum 90% code coverage requirement
- **Code Style**: Black formatter with 120-character line length
- **Type Safety**: Full type annotations with mypy validation
- **Documentation**: Comprehensive docstrings and API documentation

### Data Standards
- **Schema Versioning**: Backward-compatible schema evolution
- **Data Validation**: Strict validation with clear error messages
- **Naming Conventions**: Consistent naming across all components
- **Metadata Standards**: Rich metadata for all processed files

This project represents a critical infrastructure component for modern software development organizations, providing the foundation for advanced code analysis, architectural insights, and data-driven development practices.
